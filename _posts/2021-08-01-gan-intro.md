---
title: Basic Introduction to Generative Adversarial Networks (GANs)
tags: post machine-learning
usemathjax: false
---

# What is a Neural Network?

You may have heard of neural networks before, and you may know that they mimic brains.
Many people's knowledge on neural networks falter after that.
However, I'm here to provide a basic overview of what they are, and how they work.

A Neural Network, (or NN for short) is a model of how a brain could theoretically work.
I say model, because NNs are just complex structures made out of a bunch of tiny little parts.
In a real brain, these parts would be the neurons, and scientists have decided to name it the same thing in our model.
A NN is just a bunch of neurons connected to eachother.

Now, being modeled on the computer and restrained by the digital nature of silicon logic, it would take a lot of computation to simulate the exact voltages being pulsed through each neuron.
So, scientists have simplified it down to a bunch of linear equations, generally represented by vectors and matrices, carried out through dot products.

Why are we able to make approximations of neurons using linear algebra? Well, consider the structure of neurons in a real, human brain.

<!-- Image of neurons in a human brain -->

Here, you can see how each neuron is connected to many other neurons, with varying connection "intensities".
We can make our own model with similar properties, which can be visualized as the following.

<!-- Image of neurons in a neural network -->

Notice the similar properties here.
- Each neuron is connected to a number of other neurons.
- Each connection has a certain "intensity" or "weight".

Also, notice some other properties.
- Information/voltages/signals from each neuron only flows from left to right.
- There are input neurons.
- There are output neurons.
- The neurons in vertical columns can be grouped together and called "layers", representing each "step" of getting an output.

Modeling a network like this makes it much easier to find definite inputs and outputs, much less calculate outputs from certain inputs.
So, how can we calculate some outputs?
First, take some inputs, as a vector to represent each voltage level in the input layer.
Each connection/edge has a weight/intensity, so how can we represent the voltage encountering such resistance?
We can multiply the voltage by the edge weight.
This effectively represents the lessened/heightened voltage due to the connection strength.

Once we get all the voltages at the end of each connection, we need a way to combine them as input to the next node.
The easiest way to do that is just to get the arithmetic sum.
Now, we have the voltages of the first layer. Hooray!
In order to keep going, we repeat the process.

1. Multiply the input values by the edge weights.
2. Add them up, and set the sum as the inputs to the next neuron.

It's that simple!
One who is familiar with linear algebra may recognize the act of adding a bunch of products.
That's right; we can do this all with matrices, dot products, and matrix multiplication!
We have a vector of all the neuron inputs.
The weights corresponding to each output neuron can be put into a matrix, where every row corresponds to a different input neuron, and every column corresponds to a different output neuron.
Multiplying these two matrices will multiply each input value with each edge weight, sum them up nicely, and store it in a vector.
Linear algebra basically just served us neural networks on a silver platter!

# What is Deep Learning?

# Convolutional Neural Networks (CNNs)

# Generative Networks

# CNNs as Generative Networks

# Discriminator Networks

# CNNs as Discriminator Networks

# Duality

# Why GANs are so Powerful

# Real World Examples of GANs

# The End
